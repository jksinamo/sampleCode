---
title: "Experiment 1"
author: "Joshua Sinamo"
date: "5/19/2020"
output: html_document

---

## Appendix Overview 
- Preface & List of Security Questions
- External Requirements
- General Data Cleaning for All Analysis
- Finn and Servoss Question Set - Initialization
- My Alternative Rasch Model - Initialization
- Hyperparameter Tuning - Computation
- Hyperparameter Tuning - Visualization (AIC, BIC, RMSE, Mean of RanEf coeff)
- Hyperparameter Tuning - Visualization (Fixed Effect Coefficients)
- Assessment of Indexing Methods between The Two Models 
- Model Comparison via Bootstrapping - Initialization
- Model Comparison via Bootstrapping - Data Cleaning
- Model Comparison via Bootstrapping - Data Cleaning
- Expected Coverage Length Comparison
- Future Analysis 

## Preface & List of Security Questions
This is a mini assessment comparing two IRT (or Rasch) models involving two different sets of questions. These models are purposed for calculation, comparison and indexing of security measures across U.S. high schools. The only difference between the two models are the set of questions that are used to measure security level. Finn & Servoss (2014) in their paper: Misbehavior, Suspensions and Security Measures in High School: Racial/Ethnic and Gender Differences, proposed a set of 7 questions to measure security level. Qualitatively, few these questions seems redundant and therefore I proposed an alternative set selecting questions that seems unrelated with each other. Finn & Servoss used the Education Longitudinal Study (NCES, 2002) survey data, which is available via NCES website. In ELS:2002 survey, school administrators were asked about their schools' demographics, academic performance, and among other things security measures in their institution. 

The 24 security questions from ELS:200 Admin Questionnaire are as follows: 
#---------------------------------------------------------------------------------------------------------
38. During this school year (2001-2002), is it a practice of your school to do the following? (If your school changed its practices in the middle of the school year, please answer regarding your most recent practice.) 
(MARK ONE RESPONSE ON EACH LINE) (Yes; No) 

A.	Control access to buildings during school hours	
B.	Control access to grounds during school hours	
C.	Require students pass through metal detector	
D.	Random metal detector checks on students	
E.	Close campus for students during lunch	
F.	Random dog sniffs to check for drugs	
G.	Random sweeps for contraband	
H.	Require drug testing for any students	
I.	Require students to wear uniforms	
J.	Enforce strict dress code	
K.	Require clear book bags/ban book bags	
L.	Require students to wear badges/picture ID	
M.	Require faculty/staff to wear badges/picture ID	
N.	Use security cameras to monitor school	
O.	Telephones in most classrooms	
P.	Emergency call button in classrooms	


39. Which of the following does your school do to involve or help parents deal with school discipline issues? 
(MARK ONE RESPONSE ON EACH LINE) (Yes; No) 

A.  Process to get parent input on discipline policies
B. 	Training parents to deal with problem behavior	
C. 	Program involves parents in school discipline	


40. During the 2001-2002 school year, did your school regularly use paid law enforcement or security services at school at the following times? (MARK ONE RESPONSE ON EACH LINE) (Yes; No)

A. 	Use paid security at any time during school hours	
B.	Use paid security as students arrive or leave	
C.	Use paid security at school activities	
D.	Use paid security outside of school hours/activities	
E.	Use paid security at other time	
#---------------------------------------------------------------------------------------------------------

## Methods Sources
  On Security Questions and Literature
- Finn, Jeremy D. & Servoss, Timothy J. (2014). Misbehavior, Suspensions, and Security Measures in High Schools: 
    Racial/Ethnic and Gender Differences. Journal of Applied Research on Children Volume 5 Issue 2
    https://digitalcommons.library.tmc.edu/cgi/viewcontent.cgi?article=1211&context=childrenatrisk
    
  On Item Response Theory using LMER package in R
- Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A., Tuerlinckx, F. Partchev, I. (2011). 
    The Estimation of Item Response Models With the lmer Function from the lme4 Package in R. 
    Journal of Statistical Software Vol. 39, Issue 12.
    
  On I

## External Requirements
What  : Loading required libraries & dataset 
```{r}
# to clean up global environment, run this
rm(list = ls())
```

```{r message=FALSE}
library(lme4)     ; library(tidyr)
library(dplyr)    ; library(ggplot2) 
library(reshape2) ; library(Kendall)
library(parallel) ; library(gplots)
library(graphics) ; library(dendextend)
library(grid)         ; library(directlabels)
library(RColorBrewer) ; library(coxed)

dat <- read.csv("./Downloads/project/els_02_12_byf3pststu_v1_0.csv")
```

## General Data Cleaning for All Analysis
What : Data cleaning in accordance to NCES ELS 2002 codebook
```{r}
dataSecurity <- dat %>% dplyr::select("F1SCH_ID", "BYA38A", "BYA38B", "BYA38C", "BYA38D", "BYA38E", "BYA38F",
                                                "BYA38G", "BYA38H", "BYA38I", "BYA38J", "BYA38K", "BYA38L",
                                                "BYA38M", "BYA38N", "BYA38O", "BYA38P", "BYA39A", "BYA39B",
                                                "BYA39C", "BYA40A", "BYA40B", "BYA40C", "BYA40D", "BYA40E")
dataSecurity <- unique(dataSecurity[dataSecurity$F1SCH_ID > 1000 &
                                    dataSecurity$BYA38A != -8 & 
                                    dataSecurity$BYA38A != -7 &
                                    dataSecurity$BYA38A != -4, ])
names(dataSecurity)[2:25] <- substring(colnames(dataSecurity[,c(2:25)]), first = 4) 
names(dataSecurity)[1] <- "school"
rownames(dataSecurity) <- NULL
dataSecurity[dataSecurity == -9] <- NA
head(dataSecurity)
```


## Finn and Servoss Question Set - Initialization
What : Imitating Finn & Servoss' set of questions & transform the data into item-response set
Why  : Assess & compare the two models later (1)
How  : Literature analysis from Finn & Servoss (2014)

List of Finn & Servoss' Selected Questions:

38. During this school year (2001-2002), is it a practice of your school to do the following? (If your school changed its practices in the middle of the school year, please answer regarding your most recent practice.) 
(MARK ONE RESPONSE ON EACH LINE) (Yes; No) 

C.	Require students pass through metal detector	
D.	Random metal detector checks on students	
F.	Random dog sniffs to check for drugs	
G.	Random sweeps for contraband	
H.	Require drug testing for any students	
N.	Use security cameras to monitor school	

40. During the 2001-2002 school year, did your school regularly use paid law enforcement or security services at school at the following times? (MARK ONE RESPONSE ON EACH LINE) (Yes; No)

A. 	Use paid security at any time during school hours	

```{r}
# FINN AND SERVOS SET OF QUESTIONS

FandS_set <- dataSecurity %>% select("school", "38C", "38D", "38H", "38G", "38N", "40A", "38F")
FandS_rasch <- FandS_set %>% tidyr::gather("item", "response", -school)
```


## My Alternative Rasch Model - Initialization
What : Providing my set of security questions & transform the data into item-response set
Why  : Assess & compare the two models later (2)
How  : Finding 7 questions in ELS:2002 in which qualitatively less overlapping with each other

My alternative list of questions:

38. During this school year (2001-2002), is it a practice of your school to do the following? (If your school changed its practices in the middle of the school year, please answer regarding your most recent practice.) 
(MARK ONE RESPONSE ON EACH LINE) (Yes; No) 

C.	Require students pass through metal detector	
E.	Close campus for students during lunch	
H.	Require drug testing for any students	
J.	Enforce strict dress code	
K.	Require clear book bags/ban book bags	

39. Which of the following does your school do to involve or help parents deal with school discipline issues? 
(MARK ONE RESPONSE ON EACH LINE) (Yes; No) 

C. 	Program involves parents in school discipline	


40. During the 2001-2002 school year, did your school regularly use paid law enforcement or security services at school at the following times? (MARK ONE RESPONSE ON EACH LINE) (Yes; No)

E.	Use paid security at other time	

```{r}
# MY ALTERNATIVE SET OF QUESTIONS
alt_set <- dataSecurity %>% select("school", "38H", "38K", "38C", "38E", "40E", "39C", "38J")
alt_rasch <- alt_set %>% tidyr::gather("item", "response", -school)
```


## Hyperparameter Tuning - Computation
What   : Selecting the minimal nAGQ where the metrics start to stabilize;
       :   although the difference is minimal, I'm looking for precision. Also, catching failure to converge
Why    : Need appropriate nAGQ to have a more precise & unbiased model comparison (more = longer computation)
How    : Comparison of AIC, BIC, RMSE, fixed effect coefficients & means of random effect coefficients w.r.t nAGQ
```{r}
n = 20
RMSE_ori       <- rep(NA, n) ; RMSE_alt       <- rep(NA, n)
AIC_ori        <- rep(NA, n) ; AIC_alt        <- rep(NA, n)
BIC_ori        <- rep(NA, n) ; BIC_alt        <- rep(NA, n)
RANEF_mean_ori <- rep(NA, n) ; RANEF_mean_alt <- rep(NA, n)
FIXEF_ori      <- rep(NA, n) ; FIXEF_alt      <- rep(NA, n)

for (i in 1:n){
  
  tryCatch({rasch_lmer_ori_NAGQ <- glmer(response ~ 0 + item + (1|school), 
                                         family = binomial, data = FandS_rasch, nAGQ = i-1)},
           warning = function(warn2) {cat("Finn & Servoss at nAGQ = ", i-1, "\n")
                                      warning(warn2); cat("\n\n")})
  
  tryCatch({rasch_lmer_alt_NAGQ <- glmer(response ~ 0 + item + (1|school), 
                                         family = binomial, data = alt_rasch, nAGQ = i-1)},
           warning = function(warn2) {cat("Alternative at nAGQ    = ", i-1, "\n")
                                      warning(warn2); cat("\n\n")})
  
  RMSE_ori[i]  <- sqrt(mean(resid(rasch_lmer_ori_NAGQ)^2))  ; RMSE_alt[i]  <- sqrt(mean(resid(rasch_lmer_alt_NAGQ)^2))
  AIC_ori[i]   <- AIC(rasch_lmer_ori_NAGQ)                  ; AIC_alt[i]   <- AIC(rasch_lmer_alt_NAGQ)
  BIC_ori[i]   <- BIC(rasch_lmer_ori_NAGQ)                  ; BIC_alt[i]   <- BIC(rasch_lmer_alt_NAGQ)
  FIXEF_ori[i] <- list(fixef(rasch_lmer_ori_NAGQ))          ; FIXEF_alt[i] <- list(fixef(rasch_lmer_alt_NAGQ))
  RANEF_mean_ori[i] <- mean(ranef(rasch_lmer_ori_NAGQ)$school[["(Intercept)"]])     
  RANEF_mean_alt[i] <- mean(ranef(rasch_lmer_alt_NAGQ)$school[["(Intercept)"]])
}

```


## Hyperparameter Tuning - Visualization (AIC, BIC, RMSE, Mean of RanEf intercept coeff)
What   : Visualizing the results from nAGQ optimization above
Why    : To see what is the minimum value of nAGQ that can be considered as stable (1)
How    : Finding the minimal value where there's no considerable fluctuation of the aforementioned measurements
Result : 1. Number of Minimal Gauss-Hermit Quadrature needed >= 4
         2. My model has better AIC, BIC, RMSE and mean of random intercepts are closer to 0 (although negligible)
```{r}
assessment_data <- data.frame("measurement" = rep(c("RMSE", "AIC", "BIC", "Mean of RanEf"), each = 40),
           "model" = rep(c("F&S", "ALT"), each = 20, times = 4),
           "value" = c(c(RMSE_ori), c(RMSE_alt), c(AIC_ori), c(AIC_alt), c(BIC_ori), 
                     c(BIC_alt), c(RANEF_mean_ori), c(RANEF_mean_alt)),
           "nAGQ" = rep(seq(from = 0, to = 19), times = 8))

ggplot(data = assessment_data, aes(x = nAGQ, y = value, color = model)) + 
  geom_vline(xintercept = 4, color = "green") + 
  facet_grid(measurement~., scales = "free") + geom_line() + geom_point() + 
  labs(color    = "Model", caption = "Coefficients stabilizes at nAGQ >= 4",
       title    = "Selected Measurement vs # of Adaptive Gauss-Hermite Quadrature Points",
       subtitle =  "AIC, BIC, Mean of Ranef, and RMSE") + 
  theme(panel.spacing     = unit(0.20, "lines"),
        panel.border     = element_rect(color = "black", fill = NA, size = 1), 
        strip.background = element_rect(color = "black", size = 1),
        plot.caption     = element_text(hjust = 0.5, size = 9),
        plot.title       = element_text(size = 11), 
        plot.subtitle    = element_text(size = 9))
```


## Hyperparameter Tuning - Visualization (Fixed Effect Coefficients)
What   : Visualizing the fixed effect coefficients from nAGQ optimization above
Why    : To check what is the minimum value of nAGQ that can be considered as stable (2)
How    : Finding the minimal value where there's no considerable fluctuation of the fixed effects coefficients
Result : 1. Number of Minimal Gauss-Hermit Quadrature needed >= 1
       :    from the prior results, I decided to proceed with nAGQ = 4
         2. It seems that fixed effect coefficients of Finn & Servoss' model more well spread, 
            forming a reasonable hierarchy of security measures, while my alternative model seemingly concentrated
            near the meean
```{r}
FIXEF_comb_ori<- FIXEF_ori[[1]]
FIXEF_comb_alt<- FIXEF_alt[[1]]
for(i in 2:20){
  FIXEF_comb_ori <- rbind(FIXEF_comb_ori, FIXEF_ori[[i]])
  FIXEF_comb_alt <- rbind(FIXEF_comb_alt, FIXEF_alt[[i]])
}

fixef_coef_ori0 <- data.frame(FIXEF_comb_ori); 
fixef_coef_ori0$nAGQ <- seq(0,19); fixef_coef_ori0$model <- "F&S"
fixef_coef_ori <- melt(fixef_coef_ori0, id.vars = c("nAGQ", "model"))
fixef_coef_alt0 <- data.frame(FIXEF_comb_alt); 
fixef_coef_alt0$nAGQ <- seq(0,19); fixef_coef_alt0$model <- "ALT"
fixef_coef_alt <- melt(fixef_coef_alt0, id.vars = c("nAGQ", "model"))

fixef_graph_data <- rbind(fixef_coef_ori, fixef_coef_alt)
fixef_graph_data$variable <- substring(fixef_graph_data$variable, first = 5)

ggplot(data = fixef_graph_data, aes(x = nAGQ, y = value, colour = variable)) + 
  xlim(0,21) + geom_vline(xintercept = 1, color = "green") + 
  facet_grid(~model, scales = "free") + geom_line() + geom_point(size = 1) + 
  labs(color   = "Question", 
       title   = "Fixed Effects vs Number of Adaptive Gauss-Hermite Quadrature Points ",
       caption = "Fixed effect coefficients stabilizes at nAGQ = 1 (Laplace Approximation) or above") + 
  theme(plot.caption = element_text(hjust = 0.5)) + 
  geom_dl(aes(label = variable, x = 19.5), method = list(dl.combine("last.points"), cex = 0.7)) +
  scale_color_manual(values = colorRampPalette(brewer.pal(name = "Dark2", n = 8))(12))
```


## Assessment of Indexing Methods between The Two Models 
What   : Initializing the two rasch (or IRT) models and 
           transforming the data into item-response format for assessment
Why    : To compare the indexing methods of the two models (my alternative model vs Finn & Servoss') 
           and see whether the two models index schools security "level" differently 
How    : Kendal rank test and visualization of the two set of indices
Result : We can rejec
```{r}
rasch_lmer_FandS <- glmer(response ~ 0 + item + (1|school), family = binomial, data = FandS_rasch, nAGQ = 4)
rasch_lmer_alt <- glmer(response ~ 0 + item + (1|school), family = binomial, data = alt_rasch, nAGQ = 4)

cor.test(ranef(rasch_lmer_FandS)$school[["(Intercept)"]], 
         ranef(rasch_lmer_alt)$school[["(Intercept)"]], method = "kendal")
plot(rank(ranef(rasch_lmer_FandS)$school[["(Intercept)"]]), 
     rank(ranef(rasch_lmer_alt)$school[["(Intercept)"]]),
     xlab = "F&S", ylab = "Alternative",
     main = "Random Effect Indices : F&S vs Alternative")
```


## Model Comparison via Bootstrapping - Initialization
What   : Parallel nonparametric bootstrapping 
Why    : To calculate and compare the expected length of the 95% CIs, means of random effect coefficients,
           and time spent for bootstrapping the two models 
How    : Resampling bootstrap using parallel processing in R using "Parallel" package.
         I'm not using bootMer because we need to sample schools not school-answers (e.g. 
           there's a data format modification step needed to be executed after resampling but 
           before bootMer execute/update the model. 
Note   : It turned out that my model takes longer to converge
```{r}
# Initialize parallel clusters
cl <- makeCluster(detectCores()-1)
invisible(clusterEvalQ(cl, c(library(lme4), library(dplyr), library(tidyr))))
clusterSetRNGStream(cl)
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)

ori_rasch_wide <- dataSecurity %>% dplyr::select(school, "38C", "38D", "38H", "38G", "38N", "40A", "38F")
alt_rasch_wide <- dataSecurity %>% dplyr::select(school, "38H", "38K", "38C", "38E", "40E", "39C", "38J")

ori_rasch <- ori_rasch_wide %>% tidyr::gather("item", "response", -school)
rasch_lmer <- glmer(response ~ 0 + item + (1 | school), 
                             family = binomial, data = ori_rasch, nAGQ = 4)

resamplerWide = function(i, questionSet){
  whichrows <- sample(1L:nrow(questionSet), nrow(questionSet), replace = TRUE)
  bootspl <- questionSet[whichrows,] %>% tidyr::gather("item", "response", -school)
  refitted.mod <- update(rasch_lmer, data = bootspl)
  ranef(refitted.mod)
}

clusterExport(cl, c("ori_rasch_wide", "alt_rasch_wide", "rasch_lmer"))

bootreps <- 1000; cat("B = ", bootreps, "\n")
start.time1 <- Sys.time()
bootstats_ori  <- parSapply(cl, 1:bootreps, FUN = resamplerWide, questionSet = ori_rasch_wide)
end.time1 <- Sys.time(); cat("Finn & Servoss : Processing Time =  ", end.time1 - start.time1, " min\n")

start.time2 <- Sys.time()
bootstats_alt  <- parSapply(cl, 1:bootreps, FUN = resamplerWide, questionSet = alt_rasch_wide)
end.time2 <- Sys.time(); cat("Alternative    : Processing Time =  ", end.time2 - start.time2, " min\n")

stopCluster(cl)
```


## Model Comparison via Bootstrapping - Data Cleaning & Confidence Interval
What   : Data cleaning of the bootstrapped data and calculations of 95% bias-corrected and accelerated CI length
Why    : The bootsrapped data in replicate is in a form of list of multiple dataframes, need to combine them into one
           to then compute the confidence intervals of our random intercepts
How    : Simple dplyr group by and summarise for confidence interval using 
           bias-corrected and accelerated confidence intervals (DiCiccio & Efron, 1996).
           Implementation in R (bca) from Kropko & Harden (package = "coxed"; 2014)
````````````````````````````````````````````````````````````````````````````````````````````````````````````{r}
# "pasting" other bootstrap results into the data
dataCleaning <- function(bootstrappedData) { 
  
  lmerBootContainer  <- data.frame(bootstrappedData[1])
  lmerBootContainer$index <- rownames(lmerBootContainer)
  lmerBootContainer$rep  <- 1
  colnames(lmerBootContainer)[1] <- "value"
  for (i in 2:bootreps){
    lmerBootAdd <- data.frame(bootstrappedData[i])
    lmerBootAdd$index <- rownames(lmerBootAdd)
    lmerBootAdd$rep <- i
    colnames(lmerBootAdd)[1] <- "value"
    lmerBootContainer <- rbind(lmerBootContainer, lmerBootAdd)
  }

  # statistics for the bootstrap coefficients
  lmerBoot  <- lmerBootContainer %>% dplyr::group_by(index) %>% 
                  dplyr::summarise("mean" = mean(value, na.rm = TRUE),
                                   "LB_BCA_CI" = bca(value, conf.level = 0.95)[1],
                                   "UB_BCA_CI" = bca(value, conf.level = 0.95)[2],
                                   "CI_length" = UB_BCA_CI - LB_BCA_CI)
  return(list(lmerBootContainer, lmerBoot))
}

ori_boot <- dataCleaning(bootstats_ori)
alt_boot <- dataCleaning(bootstats_alt) 
```


## Expected CI Length Comparison
What    : Comparing the mean of the length of 95% confidence intervals
Results : On average, security indices produced using Finn & Servoss' model is 4.6% more precise than mine in a sense
            that on average their CIs are 4.6% tighter
```{r}
comparison_fands <- ori_boot[[2]][order(ori_boot[[2]]$LB_BCA_CI), ]
comparison_alt   <- alt_boot[[2]][order(alt_boot[[2]]$LB_BCA_CI), ]
mean(alt_boot[[2]]$CI_length)/ mean(ori_boot[[2]]$CI_length)
```


## Future Analysis 
Takeaways: My model performs quite well against Finn & Servoss' with better AIC, BIC and RMSE under 4 gaussian
             quadrature points. However, my model also take longer to converge and on average 4% less precise
             in its indexing process. In further investigation, we can calculate how many statistically significant
             comparison possible among the two models (e.g. how many set of bootstrapped random intercept CIs 
             that do not overlap with each others)

Based on our previous analyses, it seems that constructing questions that forms hierarchy 
  (e.g. pick n questions that ranges from most common among schools to the least common) or 
  finding questions in quantitatively the least overlapping might be working. 
  Previously, we chose the questions based on literature analysis (e.g. what are the 
  security questions that seemingly the least overlapping with each other)
Potential ways: 
    - use k-means for n groups and then pick one questions from each of the groups as a potential set
    - order the questions ascendingly w.r.t how many schools answer yes, split them them into n groups
      and then select kth question in each group
    - Find questions that quantitatively "more different" than others w.r.t responses it received (heatmap below)
```{r}
heatData <- dataSecurity[,2:25] 
heatMatAll <- matrix(ncol=24, nrow=24)
for (i in 1:24){ 
  for (j in 1:24){
    heatMatAll[i,j] <- mean(heatData[,i] == heatData[,j], na.rm = TRUE)
  }
}
colnames(heatMatAll) <- colnames(heatData)
rownames(heatMatAll) <- colnames(heatData)

heatMatAll1 <- heatMatAll
heatMatAll1[upper.tri(heatMatAll1)] <- NA
ggplot(melt(heatMatAll1, na.rm= TRUE), aes(x = Var1, y = Var2, fill = value)) + 
  geom_raster() + labs(x = "", y = "", fill = "Proportion", 
                       title = "Proportion of Same Answers Between Questions (ELS: 2002)") + 
  scale_fill_distiller(palette = "Spectral") + 
  theme_bw() 
```


